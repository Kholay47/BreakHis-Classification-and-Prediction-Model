{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809107b9",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a130a74",
   "metadata": {},
   "source": [
    "evaluation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637dff96",
   "metadata": {},
   "source": [
    "Classification Report:\n",
    "                     precision    recall  f1-score   support     \n",
    "\n",
    "           adenosis       0.78      0.85      0.82        68     \n",
    "   ductal_carcinoma       0.83      0.90      0.86       519     \n",
    "       fibroadenoma       0.62      0.80      0.70       153     \n",
    "  lobular_carcinoma       0.66      0.59      0.62        95     \n",
    " mucinous_carcinoma       0.88      0.43      0.58       120     \n",
    "papillary_carcinoma       0.56      0.64      0.60        84     \n",
    "    phyllodes_tumor       0.75      0.35      0.48        69     \n",
    "    tubular_adenoma       0.72      0.76      0.74        86     \n",
    "\n",
    "           accuracy                           0.75      1194     \n",
    "          macro avg       0.73      0.67      0.67      1194     \n",
    "       weighted avg       0.76      0.75      0.74      1194 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8724e3b",
   "metadata": {},
   "source": [
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e45b2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm  # <-- added\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"dataset_split\"\n",
    "    batch_size = 32\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "    num_classes = 8\n",
    "\n",
    "    data_transforms = {\n",
    "        \"train\": transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        ),\n",
    "        \"val\": transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        ),\n",
    "        \"test\": transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(\"Loading datasets...\")\n",
    "    image_datasets = {\n",
    "        x: datasets.ImageFolder(root=f\"{data_dir}/{x}\", transform=data_transforms[x])\n",
    "        for x in [\"train\", \"val\", \"test\"]\n",
    "    }\n",
    "\n",
    "    dataloaders = {\n",
    "        x: DataLoader(\n",
    "            image_datasets[x],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=(x == \"train\"),\n",
    "            num_workers=4,\n",
    "        )\n",
    "        for x in [\"train\", \"val\", \"test\"]\n",
    "    }\n",
    "\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"val\", \"test\"]}\n",
    "    class_names = image_datasets[\"train\"].classes\n",
    "\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    print(f\"Train size: {dataset_sizes['train']}\")\n",
    "    print(f\"Val size: {dataset_sizes['val']}\")\n",
    "    print(f\"Test size: {dataset_sizes['test']}\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            print(\"-\" * 10)\n",
    "\n",
    "            for phase in [\"train\", \"val\"]:\n",
    "                if phase == \"train\":\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Wrap dataloader with tqdm for progress bar\n",
    "                loop = tqdm(dataloaders[phase], desc=f\"{phase} batches\", leave=False)\n",
    "\n",
    "                for inputs, labels in loop:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == \"train\"):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        if phase == \"train\":\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                    # Update tqdm bar postfix with loss and accuracy so far\n",
    "                    loop.set_postfix(\n",
    "                        loss=loss.item(),\n",
    "                        acc=(torch.sum(preds == labels.data).item() / inputs.size(0)),\n",
    "                    )\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "                if phase == \"val\" and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print(f\"\\nBest val Acc: {best_acc:.4f}\")\n",
    "\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        return model\n",
    "\n",
    "    trained_model = train_model(model, criterion, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "    torch.save(trained_model.state_dict(), \"model1.pth\")\n",
    "    print(\"Training complete.model saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff040a",
   "metadata": {},
   "source": [
    "## Model 2 (ResNet 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63feff1a",
   "metadata": {},
   "source": [
    "evaluation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492a1334",
   "metadata": {},
   "source": [
    "Classification Report:\n",
    "                     precision    recall  f1-score   support\n",
    "\n",
    "           adenosis       0.94      0.96      0.95        68     \n",
    "   ductal_carcinoma       0.92      0.92      0.92       519     \n",
    "       fibroadenoma       0.88      0.93      0.90       153     \n",
    "  lobular_carcinoma       0.65      0.63      0.64        95     \n",
    " mucinous_carcinoma       0.92      0.92      0.92       120     \n",
    "papillary_carcinoma       0.91      0.92      0.91        84     \n",
    "    phyllodes_tumor       0.91      0.90      0.91        69     \n",
    "    tubular_adenoma       1.00      0.90      0.94        86     \n",
    "\n",
    "           accuracy                           0.90      1194     \n",
    "          macro avg       0.89      0.88      0.89      1194     \n",
    "       weighted avg       0.90      0.90      0.90      1194 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de78dd4c",
   "metadata": {},
   "source": [
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b54a9e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"dataset_split\"\n",
    "    batch_size = 32\n",
    "    num_epochs = 25\n",
    "    learning_rate = 0.001\n",
    "    num_classes = 8\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    data_transforms = {\n",
    "        \"train\": transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        ),\n",
    "        \"val\": transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        ),\n",
    "        \"test\": transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(\"Loading datasets...\")\n",
    "    image_datasets = {\n",
    "        x: datasets.ImageFolder(root=f\"{data_dir}/{x}\", transform=data_transforms[x])\n",
    "        for x in [\"train\", \"val\", \"test\"]\n",
    "    }\n",
    "    dataloaders = {\n",
    "        x: DataLoader(\n",
    "            image_datasets[x],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=(x == \"train\"),\n",
    "            num_workers=4,\n",
    "        )\n",
    "        for x in [\"train\", \"val\", \"test\"]\n",
    "    }\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"val\", \"test\"]}\n",
    "    class_names = image_datasets[\"train\"].classes\n",
    "\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    print(f\"Train size: {dataset_sizes['train']}\")\n",
    "    print(f\"Val size: {dataset_sizes['val']}\")\n",
    "    print(f\"Test size: {dataset_sizes['test']}\")\n",
    "\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Freeze all layers except final FC initially\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            print(\"-\" * 10)\n",
    "\n",
    "            for phase in [\"train\", \"val\"]:\n",
    "                if phase == \"train\":\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == \"train\"):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        if phase == \"train\":\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "                if phase == \"val\" and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Unfreeze all layers for fine-tuning after 10 epochs\n",
    "            if epoch == 9:\n",
    "                print(\"Unfreezing all layers for fine-tuning...\")\n",
    "                for param in model.parameters():\n",
    "                    param.requires_grad = True\n",
    "                optimizer = optim.Adam(model.parameters(), lr=learning_rate / 10)\n",
    "                scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                    optimizer, step_size=7, gamma=0.1\n",
    "                )\n",
    "\n",
    "        print(f\"\\nBest val Acc: {best_acc:.4f}\")\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        return model\n",
    "\n",
    "    trained_model = train_model(\n",
    "        model, criterion, optimizer, scheduler, num_epochs=num_epochs\n",
    "    )\n",
    "\n",
    "    torch.save(trained_model.state_dict(), \"model2.pth\")\n",
    "    print(\"Training complete\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
